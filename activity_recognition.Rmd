---
title: "Human Activity Recognition"
author: "Sergey Cherkasov"
date: "14 Oct 2015"
output: html_document
---

#Synopsis 
 
Activity trackers make possible to collect a large amount of data about personal activity. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.
 
In this paper we are going to build a model that can predict the manner in which people did the exercise. We use  dataset, generously provided by Groupware@LES in [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201).

#Exploratory analysis and cleaning data   

Training set can be found [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv).

```{r echo=TRUE}
training <- read.csv("pml-training.csv")
```

It contains `r dim(training)[1]` observations with `r dim(training)[2]` variations each. First seven variations which are **`r names(training)[1:7]`** are descriptive, so we can get rid of them.

```{r echo=TRUE}
training <- training[-(1:7)]
```

Now let us check the NAs in the data.

```{r echo=TRUE}
## Here we reveal columns with lot of NAs
na_count <-sapply(training, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
unique(na_count$na_count)
```

There are **`r length(which(na_count == unique(na_count$na_count)[2]))`** columns with **`r round(unique(na_count$na_count)[2]/dim(training)[1]*100, digits=0)`%** of NAs. We can remove them too. All other columns have no NAs. We convert everything but the variation **classe** to numeric. We also remove all zero covariates. And finally we split training data set for two sets (90%-10%) for validating the model and plug in caret package.

```{r echo=TRUE}
## Removing columns with a lot of NAs.
n <- which(na_count == 19216)
training <- training[-n]

## Convert to numeric all columns but "classe"
last<-dim(training)[2]
training[, -last] <- sapply(training[, -last] , function(x) as.numeric(x))

##Removing zero covariates
library(caret, quietly=TRUE)
nsv <- nearZeroVar(training[,-last],saveMetrics=TRUE)
training <- training[,!nsv$nzv]

## Building two sets
set.seed(1234)
inTrain <- createDataPartition(training$classe, p=0.90, list = FALSE)
trTrain <- training[inTrain,]
trTest <- training[-inTrain,]
```

#Building the model 

Now we can build the model. We are going to use Random Forest algorithm. It was created by Leo Breiman and Adele Cutler and has many advantages. As it stated [here](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm) it is unexcelled in accuracy among current algorithms, it gives estimates of what variables are important in the classification. And there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run. Caret package provides a number of methods to estimate the accuracy of a machines learning algorithm. We are going to use k-folds validation with k=5.

```{r echo=TRUE}
ctrl <- trainControl(allowParallel = TRUE, method = "cv", number = 5)
fit <- train(classe ~ ., data = trTrain, method="rf", trainControl = ctrl)
```



#Cross validation and model evaluation

Now we can estimate the error of the model, using our reserved data. It is those 10% separated and independent percent of data. Error we evaluate is actually the k-fold validation error due to feature of our algorithm.

```{r echo=TRUE}
err <- confusionMatrix(trTest$classe, predict(fit, trTest))
err
```

Out of sample error can be calculating by subtraction the model accuracy out of one. For this model it is **`r round((1 - err$overall[1])*100, digits=2)`%**.


#Final classification 

Now it is time to use provided test set with 20 obrevations to predict variation "classe". First of all we tidy the provided data same way as we did with training set and then we apply the model to the test set. Those results were 100% accurate at the course project submission. They are not shown to prevent the violation of Honor Code.